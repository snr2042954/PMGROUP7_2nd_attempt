\subsection{Results}

\textbf{Baseline Comparison (Requirement i)}: The PM4Py $\alpha$-miner outperformed our reimplementation overall, with an average F1-score of 0.88 compared to 0.84 for the default custom miner (abs=0, rel=0.0). PM4Py's $\alpha$-miner achieved higher F1-scores on 7 out of 12 datasets, while our implementation performed better on 2 datasets (\textit{L7.xes} and \textit{flyerinstances.xes}) and tied on 3 datasets (\textit{L4.xes}, \textit{billinstances.xes}, and \textit{posterinstances.xes}). The performance difference stems primarily from additional false-positive relations in our implementation (e.g., L1: 2 FP vs 0 FP, L2: 2 FP vs 0 FP), not from missing behavior. For several datasets (e.g., \textit{billinstances.xes}, \textit{L4.xes}, \textit{posterinstances.xes}), both miners achieved identical F1-scores of 1.0, confirming that our version faithfully reproduces the formal $\alpha$-miner behavior. Notably, on \textit{L7.xes}, our implementation achieved perfect F1 (1.000) while PM4Py's $\alpha$-miner only reached 0.400 due to missing 3 out of 4 gold standard relations, demonstrating superior handling of short loops.\newline

\textbf{Parameter Sensitivity (Requirement ii)}: The parameter grid search explored ten absolute frequency thresholds ($f \in \{1,\dots,10\}$) and eleven relative frequency thresholds ($r \in \{0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5\}$), totaling 110 combinations per dataset. Across datasets, the best configurations consistently used minimal absolute thresholds ($f = 1$) and zero relative thresholds ($r = 0.0$), with only two exceptions: \textit{L5.xes} used $f = 8, r = 0.0$ and \textit{running-example.xes} used $f = 1, r = 0.15$. This finding is counterintuitive, as one might expect higher thresholds to suppress noise more effectively. It indicates that, in practice, the combination of the $\alpha$-miner's strict causality definition and the dependency measure already eliminates most weak or accidental relations. Consequently, additional frequency filtering contributes little beyond reinforcing the miner's inherent robustness, even on complex real-world data. Comparing the best tuned custom miner (average F1: 0.87) against PM4Py's $\alpha$-miner (average F1: 0.88), PM4Py still maintains a slight advantage, winning on 7 datasets, tying on 3, and losing on 2. However, the gap narrows significantly compared to the default custom miner, demonstrating that parameter tuning helps our implementation approach PM4Py's performance.\newline

\textbf{Comparison with Heuristics Miner (Requirement iii)}: Using the best tuned parameters, the enhanced $\alpha$-miner achieved equal or superior F1-scores on all twelve datasets compared to PM4Py's Heuristics Miner. It outperformed Heuristics on 4 datasets (\textit{L5.xes}, \textit{L7.xes}, \textit{running-example.xes}, and \textit{BPI\_Challenge\_2012.xes}) and tied on the remaining 8 datasets. No dataset favored the Heuristics Miner, confirming that the enhanced $\alpha$-miner combines $\alpha$-miner precision with heuristics-style robustness. The enhanced $\alpha$-miner achieved an average F1-score of 0.87 compared to 0.83 for the Heuristics Miner. On \textit{BPI\_Challenge\_2012.xes}, the enhanced $\alpha$-miner obtained a precision of 0.35 and a recall of 0.45 (F1: 0.39), while the Heuristics Miner achieved precision 0.10 and recall 0.50 (F1: 0.16). This reflects that the enhanced $\alpha$-miner produces more conservative but more precise models, whereas the Heuristics Miner captures more relations but with lower precision. The enhanced $\alpha$-miner's superior precision (0.35 vs 0.10) demonstrates its ability to filter out spurious relations even on complex real-world logs.

\begin{table}[h!]
\centering
\caption{Summary of Key Quantitative Results Across All Evaluations}
\label{tab:summary_results}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lccc}
\toprule
\textbf{Comparison} & \textbf{Avg.\ F1 (Baseline)} & \textbf{Avg.\ F1 (Enhanced)} & \textbf{Outcome}\\
\midrule
PM4Py $\alpha$ vs.\ Default Custom (Req.\ i) & 0.88 & 0.84 & PM4Py wins (7/12), Ties (3/12)\\
PM4Py $\alpha$ vs.\ Best Custom (Req.\ ii) & 0.88 & 0.87 & PM4Py wins (7/12), Ties (3/12)\\
Enhanced $\alpha$ vs.\ Heuristics (Req.\ iii) & 0.83 & 0.87 & Enhanced wins (4/12) + 8 ties\\
\midrule
\textbf{Total Evaluations} & \multicolumn{3}{c}{1,320 grid search runs + 48 baseline evaluations = 1,368 total}\\
\bottomrule
\end{tabular}%
}
\end{table}

Despite the comprehensive grid search (1,320 total evaluations across 12 datasets), the full experiment completed within hours on standard hardware (with most of the time spent on the \textit{BPI\_Challenge\_2012.xes} dataset, which required 372 seconds for grid search alone), demonstrating the scalability of the evaluation framework. The confusion matrix metrics reveal that our implementation's lower F1-scores compared to PM4Py's $\alpha$-miner stem from false positives rather than false negatives: for example, on L1, we correctly identify all 6 gold relations (TP=6, FN=0) but also include 2 spurious relations (FP=2), whereas PM4Py achieves perfect precision (TP=6, FP=0). This pattern suggests that our implementation may be slightly more permissive in relation detection, though it maintains perfect recall across most datasets.

\subsection{Summary of Findings}

\begin{itemize}
    \item \textbf{Gold standards} provided an objective reference for all evaluations, ensuring that precision, recall, and F1-scores accurately reflect structural correctness. Confusion matrix metrics (TP, FP, FN, TN) revealed that performance differences stem primarily from false positives rather than missed relations.
    
    \item \textbf{Enhanced $\alpha$-miner} consistently matched or surpassed the Heuristics Miner (average F1: 0.87 vs 0.83) while preserving Petri-net interpretability and achieving superior precision on complex real-world logs.
    
    \item \textbf{Parameter tuning} (10$\times$11 grid; 110 configs/dataset) revealed that optimal configurations consistently use minimal thresholds ($f=1, r=0.0$), indicating that the $\alpha$-miner's inherent structure already provides effective filtering. Tuning provided marginal gains (average F1 improvement: +0.03).
    
    \item The evaluation encompassed 1,368 reproducible runs over 12 datasets (1,320 grid search evaluations plus 48 baseline comparisons), fully automated through Python scripts with comprehensive metrics including confusion matrices.
\end{itemize}

In summary, the evaluation demonstrates that incorporating frequency-based thresholds into the $\alpha$-miner provides minimal additional benefit on these datasets, as the optimal configuration consistently uses minimal filtering ($f=1, r=0.0$). This finding indicates that the $\alpha$-miner's strict causality requirements and dependency measures already eliminate most spurious relations, making additional frequency filtering largely redundant. The gold-standard comparisons confirm that our enhanced miner faithfully captures dominant process logic while maintaining perfect recall on most datasets. Interestingly, our implementation outperformed PM4Py's $\alpha$-miner on datasets with short loops (\textit{L7.xes}) and complex loop structures (\textit{flyerinstances.xes}), suggesting improved handling of bidirectional causal relations. On the complex \textit{BPI\_Challenge\_2012.xes} dataset, the enhanced $\alpha$-miner achieved significantly higher precision than the Heuristics Miner (0.35 vs 0.10), demonstrating its ability to filter noise while maintaining reasonable recall (0.45 vs 0.50).

